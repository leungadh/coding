{
  
    
        "post0": {
            "title": "Improved search for Mirror members",
            "content": "From Model to Production . The Practice of Deep Learning . Starting Your Project . The State of Deep Learning . Computer vision . Text (natural language processing) . Combining text and images . Tabular data . Recommendation systems . Other data types . The Drivetrain Approach . Gathering Data . clean . To download images with Bing Image Search, sign up at Microsoft Azure for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;febeb1b830924132a317e052e512f951&#39;) . search_images_bing . &lt;function fastbook.search_images_bing&gt; . Path() . Path(&#39;.&#39;) . results = search_images_bing(key, &#39;grizzly bear&#39;) ims = results.attrgot(&#39;contentUrl&#39;) len(ims) . 150 . ims[0] . &#39;http://2.bp.blogspot.com/-NjMTuklENdE/UHzVv_8dIxI/AAAAAAAAA-U/tNBsQDn8kFI/s1600/Grizzly+Bear+Pic.jpg&#39; . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . . 100.96% [335872/332689 00:00&lt;00:00] Path(&#39;images/grizzly.jpg&#39;) . im = Image.open(dest) im.to_thumb(128,128) . mirror_types = &#39;姜濤&#39;,&#39;呂爵安&#39;,&#39;盧瀚霆&#39; path = Path(&#39;mirror&#39;) . path . Path(&#39;mirror&#39;) . path.exists() . False . path.mkdir() . path . Path(&#39;mirror&#39;) . path.exists() . True . Path().ls() . (#4) [Path(&#39;.config&#39;),Path(&#39;mirror&#39;),Path(&#39;gdrive&#39;),Path(&#39;sample_data&#39;)] . destkt= (path/&#39;keungto&#39;) destkt.mkdir(exist_ok=True) results = search_images_bing(key,f&#39;姜濤&#39;) urls = results.attrgot(&#39;contentUrl&#39;) #download_images(destkt,ims) . urls . (#150) [&#39;https://imgs.orientalsunday.hk/wp-content/uploads/2021/04/kt4_1115a_francis_5116146460894376f0174.jpg&#39;,&#39;https://www.mpweekly.com/entertainment/wp-content/uploads/2019/04/2019-04-056.10.18.png&#39;,&#39;https://orientaldaily.on.cc/cnt/entertainment/20181026/photo/1026-00282-055b1.jpg&#39;,&#39;https://image.topbeautyhk.com/wp-content/uploads/2020/06/79942562_173958253851421_1191074941406788964_n.jpg&#39;,&#39;https://image.topbeautyhk.com/wp-content/uploads/2020/06/60132425_1109867679220022_6550365207692793539_n.jpg&#39;,&#39;http://s3-ap-southeast-1.amazonaws.com/girls.presslogic.com/wp-content/uploads/2020/03/67954023_750995765318617_6486667687090809855_n.jpg&#39;,&#39;http://www.orientaldaily.on.cc/cnt/entertainment/20181115/photo/1115-00282-012b2.jpg&#39;,&#39;https://image.topbeautyhk.com/wp-content/uploads/2020/06/94703849_939869166446151_6479342828000137983_n.jpg&#39;,&#39;https://media.zenfs.com/en/elle_hk_497/40fc3f363663a6b19760b19299ccf6d1&#39;,&#39;https://n.kinliu.hk/wp-content/uploads/2020/12/WhatsApp-Image-2020-12-09-at-18.29.19-1-768x941.jpeg&#39;...] . destkt . Path(&#39;mirror/keungto&#39;) . download_images(destkt,urls=results.attrgot(&#39;contentUrl&#39;)) . #destel = (path/&#39;edenlui&#39;) #destal = (path/&#39;ansonlo&#39;) . destel= (path/&#39;edenlui&#39;) destel.mkdir(exist_ok=True) results = search_images_bing(key,f&#39;呂爵安&#39;) urls = results.attrgot(&#39;contentUrl&#39;) . urls . (#22) [&#39;https://hk.ulifestyle.com.hk/cms/images/topic/480x270/202105/20210512211956_4_edanlui-80714291-2924011137630799-3958674791058260682-n.jpeg&#39;,&#39;https://imgs.sundaykiss.com/wp-content/uploads/2021/05/ianfeatueimage_1859008326095793ad70f2-360x215.jpg&#39;,&#39;https://image.topbeautyhk.com/wp-content/uploads/2021/04/2e8586bf.jpeg&#39;,&#39;http://mc.scyeah.com.hk/wp-content/uploads/2018/12/img_8147.jpg&#39;,&#39;https://resource01.ulifestyle.com.hk/res/v3/image/content/2140000/2143243/20180822JLB003__20180822_L.jpg&#39;,&#39;http://static.stheadline.com/stheadline/inewsmedia/20190804/_2019080417172433832.jpg&#39;,&#39;https://hk.ulifestyle.com.hk/cms/images/topic/480x270/202105/20210513182321_0_183783349-1340552942997821-5478675196666266770-n.jpg&#39;,&#39;https://hk.ulifestyle.com.hk/cms/images/topic/480x270/202105/20210526124657_3_edanlui-146910115-3547158525393696-553924732058702664-n.jpg&#39;,&#39;https://image.stheadline.com/f/332p0/0x0/100/st/645eaad329dfb85a516dcf2bb3451a04/stheadline/inewsmedia/20200521/_2020052110001346682.jpg&#39;,&#39;https://image.stheadline.com/f/1500p0/0x0/100/st/5300c8eea04245cadd130419d636b37b/stheadline/inewsmedia/20200521/_2020052110001011343.jpg&#39;...] . destel . Path(&#39;mirror/edenlui&#39;) . download_images(destel,urls=results.attrgot(&#39;contentUrl&#39;)) . (path/&#39;edenlui&#39;).ls() . (#13) [Path(&#39;mirror/edenlui/00000005.jpg&#39;),Path(&#39;mirror/edenlui/00000001.jpg&#39;),Path(&#39;mirror/edenlui/00000002.jpeg&#39;),Path(&#39;mirror/edenlui/00000010.jpg&#39;),Path(&#39;mirror/edenlui/00000020.jpg&#39;),Path(&#39;mirror/edenlui/00000000.jpeg&#39;),Path(&#39;mirror/edenlui/00000008.jpg&#39;),Path(&#39;mirror/edenlui/00000009.jpg&#39;),Path(&#39;mirror/edenlui/00000015.jpg&#39;),Path(&#39;mirror/edenlui/00000006.jpg&#39;)...] . destal= (path/&#39;ansonlo&#39;) destal.mkdir(exist_ok=True) results = search_images_bing(key,f&#39;盧瀚霆&#39;) urls = results.attrgot(&#39;contentUrl&#39;) . urls . (#150) [&#39;https://www.elle.com.hk/var/ellehk/storage/images/celebrity/feature/good-night-show/4.-anson-lo/28863844-1-chi-HK/4.-Anson-Lo_img_885_590.jpg&#39;,&#39;https://i.ytimg.com/vi/7umY7WZyE5M/maxresdefault.jpg&#39;,&#39;https://api.elle.com.hk/var/ellehk/storage/images/fashion/mirror-12-men-uniform-look/node_1717420/29320813-1-chi-HK/Mirror-Anson-Lo_img_1040_780.jpg&#39;,&#39;https://i.ytimg.com/vi/63kYZhp-FtU/maxresdefault.jpg?v=60e543ee&#39;,&#39;https://i.ytimg.com/vi/KJDtNy8TLaA/maxresdefault.jpg&#39;,&#39;http://static.stheadline.com/stheadline/inewsmedia/20190602/_2019060215582269668.jpg&#39;,&#39;https://image.stheadline.com/f/1500p0/0x0/100/st/3b9f7f1aa9175bdac5a9eaa011e9dc77/stheadline/inewsmedia/20201202/_2020120216074575025.jpg&#39;,&#39;https://i.ytimg.com/vi/VUYiA-UHuwY/maxresdefault.jpg&#39;,&#39;http://static.stheadline.com/stheadline/news_res/2019/06/03/166550/i_640x589_174965883.jpg&#39;,&#39;https://lh6.googleusercontent.com/proxy/7tn0kNp-kmd4GTwB6JCmbUKgdGKk5Nsx-NBzxeWFR50gyWY1HUGdqqOk0VczvVOOIqWVZf7GJ_9y8xjZeUgNGe45lp0NkBDp5JJ_RUFIhUmQmQgYnHrQq9mdGx7FMrihvJp6UqsDGWbEOJabo89aRY6JbsySug=s0-d&#39;...] . destal . Path(&#39;mirror/ansonlo&#39;) . download_images(destal,urls=results.attrgot(&#39;contentUrl&#39;)) . (path/&#39;ansonlo&#39;).ls() . (#132) [Path(&#39;mirror/ansonlo/00000099.jpg&#39;),Path(&#39;mirror/ansonlo/00000146.jpg&#39;),Path(&#39;mirror/ansonlo/00000005.jpg&#39;),Path(&#39;mirror/ansonlo/00000113.jpg,zoom=600,quality=70,type=jpg&#39;),Path(&#39;mirror/ansonlo/00000059.jpg&#39;),Path(&#39;mirror/ansonlo/00000135.jpg&#39;),Path(&#39;mirror/ansonlo/00000001.jpg&#39;),Path(&#39;mirror/ansonlo/00000130.jpg&#39;),Path(&#39;mirror/ansonlo/00000138.png&#39;),Path(&#39;mirror/ansonlo/00000132.jpg&#39;)...] . path . Path(&#39;mirror&#39;) . # path.mkdir() # for o in bear_types: # dest = (path/o) # dest.mkdir(exist_ok=True) # results = search_images_bing(key, f&#39;{o} bear&#39;) # download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . fns = get_image_files(path) fns . (#281) [Path(&#39;mirror/edenlui/00000005.jpg&#39;),Path(&#39;mirror/edenlui/00000001.jpg&#39;),Path(&#39;mirror/edenlui/00000002.jpeg&#39;),Path(&#39;mirror/edenlui/00000010.jpg&#39;),Path(&#39;mirror/edenlui/00000020.jpg&#39;),Path(&#39;mirror/edenlui/00000000.jpeg&#39;),Path(&#39;mirror/edenlui/00000008.jpg&#39;),Path(&#39;mirror/edenlui/00000009.jpg&#39;),Path(&#39;mirror/edenlui/00000015.jpg&#39;),Path(&#39;mirror/edenlui/00000006.jpg&#39;)...] . failed = verify_images(fns) failed . (#3) [Path(&#39;mirror/keungto/00000071.jpg&#39;),Path(&#39;mirror/keungto/00000006.jpg&#39;),Path(&#39;mirror/ansonlo/00000140.jpg&#39;)] . failed.map(Path.unlink); . Sidebar: Getting Help in Jupyter Notebooks . End sidebar . From Data to DataLoaders . members = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = members.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . members = members.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = members.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . members = members.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = members.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . members = members.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = members.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.924048 | 2.105088 | 0.666667 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 1.685409 | 1.892800 | 0.611111 | 00:08 | . 1 | 1.525636 | 1.488628 | 0.462963 | 00:07 | . 2 | 1.355721 | 1.323670 | 0.370370 | 00:07 | . 3 | 1.233981 | 1.176192 | 0.370370 | 00:07 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . learn_inf.dls.vocab . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . Deploying your app . How to Avoid Disaster . Unforeseen Consequences and Feedback Loops . Get Writing! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out-of-domain data&quot;? | What is &quot;domain shift&quot;? | What are the three steps in the deployment process? | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://leungadh.github.io/coding/2202/04/15/improved-mirror-search.html",
            "relUrl": "/2202/04/15/improved-mirror-search.html",
            "date": " • Apr 15, 2202"
        }
        
    
  
    
        ,"post1": {
            "title": "Day 1: O'Reilly Training",
            "content": "Welcome and agenda . Getting started What is data analysis? | What is Pandas? | Descriptive statistics | Using Pandas series | Broadcasting | Mask arrays | Useful methods | . | Data types and data frames dtypes (different types of data we can store) | NaN (&quot;not a number&quot;) | Data frames | Querying with boolean/mask indexes | Reading CSV data | . | Real-world data CSV | Online data | Sorting | Grouping | Pivot tables | Joining | Cleaning data | . | Text and dates Text | Dates | . | Visualization Line plots | Bar plots | Histograms | Pie plots | Scatter plots | Boxplots | . | What is data? . If we&#39;re talking about data analysis, then we should really know what data is! . The nouns in the computer world are data. . - Files (lots of types of files) - Logs (information about who did what, and when, on which computer) - Preferences (e.g., Netflix) - Store inventories and purchasing histories . Thanks to mobile devices, and computers, and companies all getting interconnected, there is a LOT of data out there. . The problem isn&#39;t finding data. The problem is understanding what our data really means, and doing something useful with it. . The scientific method means: I ask a question, and I try to answer that question as best as possible, using techniques that others have demonstrated are reliable. Data science is all about applying that method to the world of data. . I want to be able to ask a question, and then answer it. I&#39;ll use data in order to do that. . Some examples: . - What products are selling best? - What products are selling best in each country? In each age demographic? In each country vs. each age demographic? - Which employees are bringing in the most sales? - Which universities&#39; graduates earn the most money 10 years after graduating? - Which stocks/bonds/investments have done best over the last 10 years? 50 years? . Data science . The idea is: Use scientific principles to ask questions and answer them with data. . I divide the world of data science into three pieces: . Data analysis — use the data we&#39;ve collected to understand the past and present | Data engineering — there&#39;s so much data out there, in so many different formats and sources, and getting it in a timely, organized way to our team&#39;s computers is hard -- data engineers solve these problems | Machine learning — learn from the past, to make predictions about the future | . How are we going to gather the data, and ask questions? We&#39;re going to read it into data structures on our computer, and then use methods on those data structures to create queries. We&#39;ll be using Python and Pandas in this course. . Exercise: Think about data . What data does Amazon have about its products? What data does Amazon have about you? How does Amazon use this data in its business? . Python -- why is this a language for data analytics? . It&#39;s also a very good choice: . Easy to read | Easy to learn | Lots of support | Open source (cheap or free) | . It&#39;s a very bad choice, in many ways: . It runs slowly (relative to many other languages) | It uses lots of memory | . Data analytics uses lots of data, often many hundreds of megabytes -- or more! It&#39;s not at all unusual to have data sets that are several GB in size. If the same data in Python is 10x bigger than the data in C, then you can handle more data in C, even if it&#39;s a harder language to work with. . The reason is something called &quot;NumPy.&quot; This is a Python module, written mostly in C. It allows us to work with data in C format (i.e., very fast, very small), using a very thin layer of Python on top of it. NumPy is super fast and super efficient, but it lets us work with friendly Python code. . The best of both worlds! (Almost) . NumPy can still be a bit low level and hard to work with. We, in this class, will be using Pandas. Pandas is (mostly) a wrapper around NumPy, making it far friendlier and easier to work with. . Just the Pandas library for Python has between 5-10 million users. People at companies and organizations around the world are using Pandas more and more to analyze their data: . E-commerce companies | Manufacturers | Banks and financial institutions | Marketing companies | . Data structures in Pandas . Pandas mostly ignores Python data structures, in favor of its own: . Series (1-dimensional data) | Data frame (2-dimensional data) | . Assuming that you have loaded Pandas and Jupyter onto your computer, you can say: . import numpy as np # we will use, occasionally, some of the low-level NumPy functionality via &quot;np&quot; import pandas as pd # load the Pandas module into memory, and make it available via the &quot;pd&quot; namespace from pandas import Series # I also want to use the Series name by itself, rather than pd.Series . # I use a Python list of integers to create my series s = Series([10, 20, 30, 40, 50, 60, 70]) . type(s) # in Jupyter, if an expression is on the final line of a cell, we get the value back w/o print . pandas.core.series.Series . s[0] . 10 . s[5] . 60 . t = Series([50, 40, 20, 30, 88, 22, 16]) . mylist1 = [10, 20, 30] mylist2 = [40, 50, 60] mylist1 + mylist2 # we get a new list -- all elements of mylist1, followed by all elements of mylist2 . [10, 20, 30, 40, 50, 60] . . s . 0 10 1 20 2 30 3 40 4 50 5 60 6 70 dtype: int64 . t . 0 50 1 40 2 20 3 30 4 88 5 22 6 16 dtype: int64 . # we get a new series -- the new series has 7 elements, with the index 0-6 # the new series, at index 0, is s[0] + t[0] # the new series, at index 1, is s[1] + t[1] # the addition took places as vectors! s + t . 0 60 1 60 2 50 3 70 4 138 5 82 6 86 dtype: int64 . s - t . 0 -40 1 -20 2 10 3 10 4 -38 5 38 6 54 dtype: int64 . s * t . 0 500 1 800 2 600 3 1200 4 4400 5 1320 6 1120 dtype: int64 . s / t . 0 0.200000 1 0.500000 2 1.500000 3 1.333333 4 0.568182 5 2.727273 6 4.375000 dtype: float64 . s % t # remainder from dividing s/t . 0 10 1 20 2 10 3 10 4 50 5 16 6 6 dtype: int64 . Exercise: Series operations . Create a series containing three elements — your birth year, month, and day. | Create a second series containing three elements -- birth year, month, and day -- from someone you know. | Show the difference between your two ages in year, month, day | me = Series([1970, 7, 14]) sis = Series([1971, 8, 22]) . sis - me . 0 1 1 1 2 8 dtype: int64 . me = Series([1970, 07, 14]) . File &#34;/var/folders/rr/0mnyyv811fs5vyp22gf4fxk00000gn/T/ipykernel_3159/2946636120.py&#34;, line 1 me = Series([1970, 07, 14]) ^ SyntaxError: leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers . Different number bases . If you want to enter numbers in a different number base, you can do that in Python: . base 2 (binary): Use 0b, as in 0b101010 | base 16 (hex): Use 0x, as in 0xab12cd34 | base 8 (octal): Use 0o, as in 0o12345 | . Next up: . Descriptive statistics | Aggregate methods | Five-number summary | Mean and standard deviation | . Lists vs. arrays . Python normally uses lists. Many people say to me, &quot;Oh, those are just arrays, right?&quot; No, because there are two basic differences between lists and arrays: . Arrays have a set length, that cannot be changed once they&#39;re created. | All elements in an array must be of the same type. | Since Pandas series are based on NumPy arrays, you might think that they cannot have different types in them. But it turns out that they can, thanks to a bit of Python magic! . s = Series([10, 20, 30, 40, 50]) s . 0 10 1 20 2 30 3 40 4 50 dtype: int64 . s = Series([10.5, 20.3, 30.4, 40, 50]) s . 0 10.5 1 20.3 2 30.4 3 40.0 4 50.0 dtype: float64 . s = Series([&#39;a&#39;, &#39;bc&#39;, &#39;defg&#39;, &#39;hi&#39;]) s . 0 a 1 bc 2 defg 3 hi dtype: object . # we can have a mix, then, and Pandas will live with it # but we should to avoid it s = Series([10, 20, &#39;abc&#39;, &#39;def&#39;, 30.5, 60.8, [1,2,3,4]]) . s . 0 10 1 20 2 abc 3 def 4 30.5 5 60.8 6 [1, 2, 3, 4] dtype: object . # pip install numpy # this is not a Python command -- you need to run it on the command line . # how can we describe this? s = Series([52, 50, 21, 19, 16]) . # one possibility: mean (average) -- sum, divided by the number of data points # Pandas series have a bunch of methods for this, and other *descriptive statistics*. s.mean() # mean is great... but also flawed, because it is easily skewed with one or two outliers . 31.6 . # the median is calculated in this way: Take all values, from the smallest to the largest, # and line them up, sorted. Take the middle value or (if there is an even number of values) # the average (mean) of the two middle values . s.median() . 21.0 . # In Python 3, division with / always returns a float, even if the numbers are all integers # if you want an integer result, use //, known as &quot;floordiv,&quot; which removes (not rounds!) any decimals . # My favorite way: help(function) help(s.median) . Help on method median in module pandas.core.generic: median(axis: &#39;int | None | lib.NoDefault&#39; = &lt;no_default&gt;, skipna=True, level=None, numeric_only=None, **kwargs) method of pandas.core.series.Series instance Return the median of the values over the requested axis. Parameters - axis : {index (0)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a scalar. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns - scalar or Series (if level specified) . # a standard deviation of 0 means: all data points are the same # often we talk about mean+1sd mean-1sd s.std() # calculate the standard deviation on our data . 17.812916661793487 . s.mean() . 31.6 . s.mean() + s.std() . 49.41291666179349 . s.mean() - s.std() . 13.787083338206514 . . s . 0 52 1 50 2 21 3 19 4 16 dtype: int64 . s.mean() + 2 * s.std() . 67.22583332358698 . s.mean() - 2 * s.std() . -4.025833323586973 . Descriptive statistics . We can now understand our data in a variety of ways: . Mean, which tells us the arithmetic average — easy to understand, and often useful, and the reference point for the standard deviation. However, it&#39;s easy to skew with outliers. | Standard deviation, which tells us how much the data spreads out from the mean. A high std means that the data is all over the place, whereas a low one means that it&#39;s concentrated around the mean. | Median, which is the middle value. | . You can also think of the median as the 50% point in our data, if we line things up from lowest to highest. We can similarly calculate the 25% mark and the 75% mark in our data. These are often known as the 1Q and 3Q values. Pandas lets us calculate these pretty easily: . s.quantile(0.25) . 19.0 . s.quantile(0.5) # median . 21.0 . s.quantile(0.75) . 50.0 . s . 0 52 1 50 2 21 3 19 4 16 dtype: int64 . . s.min() . 16 . s.max() . 52 . Descriptive statistics summary . For a series s, we can call: . s.mean() | s.std() | s.quantile(0.25) | s.quantile(0.50) or s.median() | s.quantile(0.75) | s.min() | s.max() | . Exercise: Weather forecast . Go to a Web site that shows the 10-day forecast for your area, Create a series containing the high temperatures for each day in the next 10 days. | Calculate each of the descriptive statistics. Are there are any obvious outliers (very hot or very cold) in the coming 10 days? | help(s.quantile) . Help on method quantile in module pandas.core.series: quantile(q=0.5, interpolation=&#39;linear&#39;) method of pandas.core.series.Series instance Return value at the given quantile. Parameters - q : float or array-like, default 0.5 (50% quantile) The quantile(s) to compute, which can lie in range: 0 &lt;= q &lt;= 1. interpolation : {&#39;linear&#39;, &#39;lower&#39;, &#39;higher&#39;, &#39;midpoint&#39;, &#39;nearest&#39;} This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points `i` and `j`: * linear: `i + (j - i) * fraction`, where `fraction` is the fractional part of the index surrounded by `i` and `j`. * lower: `i`. * higher: `j`. * nearest: `i` or `j` whichever is nearest. * midpoint: (`i` + `j`) / 2. Returns - float or Series If ``q`` is an array, a Series will be returned where the index is ``q`` and the values are the quantiles, otherwise a float will be returned. See Also -- core.window.Rolling.quantile : Calculate the rolling quantile. numpy.percentile : Returns the q-th percentile(s) of the array elements. Examples -- &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4]) &gt;&gt;&gt; s.quantile(.5) 2.5 &gt;&gt;&gt; s.quantile([.25, .5, .75]) 0.25 1.75 0.50 2.50 0.75 3.25 dtype: float64 . s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32]) . s.min() . 32 . s.max() . 34 . s.mean() . 33.2 . s.median() . 33.5 . s.std() . 0.9189365834726815 . s.mean() - s.std() . 32.281063416527324 . s.mean() + s.std() . 34.11893658347268 . s.describe() . count 10.000000 mean 33.200000 std 0.918937 min 32.000000 25% 32.250000 50% 33.500000 75% 34.000000 max 34.000000 dtype: float64 . np.random.seed(0) # always start random numbers from the same place, so we all get the same values s = Series(np.random.randint(0, 100, 10)) # get 10 random ints from 0-100, and create a series from them . s . 0 44 1 47 2 64 3 67 4 67 5 9 6 83 7 21 8 36 9 87 dtype: int64 . # but it&#39;s much better to use s.loc[n] s.loc[5] . 9 . # outer ones are for s.loc # inner ones mean: I&#39;m asking for 3 values s.loc[[3, 5, 7]] # this is a &quot;fancy&quot; index, where I ask for three different values . 3 67 5 9 7 21 dtype: int64 . s.loc[4:7] # starting at 4, until 7 . 4 67 5 9 6 83 7 21 dtype: int64 . s.loc[5] = 999 s . 0 44 1 47 2 64 3 67 4 67 5 999 6 83 7 21 8 36 9 87 dtype: int64 . s.loc[[3,5,7]] = 888 . s . 0 44 1 47 2 64 3 888 4 67 5 888 6 83 7 888 8 36 9 87 dtype: int64 . s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32]) . s . 0 33 1 34 2 34 3 34 4 34 5 34 6 33 7 32 8 32 9 32 dtype: int64 . # For example, MMDD # Pandas allows for this! # pass index = and a list of values (strings, integers, anything!) s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ]) . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . s.loc[&#39;0809&#39;] . 32 . s.loc[&#39;0808&#39;:&#39;0810&#39;] # use a slice . 0808 33 0809 32 0810 32 dtype: int64 . s.loc[[&#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;]] . 0808 33 0809 32 0810 32 dtype: int64 . s.loc[[&#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;]] = 98,99,100 . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 98 0809 99 0810 100 0811 32 dtype: int64 . s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ]) . Retrieving via indexes . We&#39;ve now seen that we can use .loc to retrieve one or more values from our series, using the index. If the index is the default (numbers), we can use it via the positions. But if the index is custom, either strings or integers, we can still use it. . But what if we want to retrieve by position, even though we have our own string index? We can do that with the .iloc accessor. It works just like .loc, but uses the numeric positions (starting at 0), rather than our own custom index. . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . s.loc[&#39;0804&#39;] # retrieving via .loc and the index . 34 . s.iloc[2] # retrieving via .iloc and the position . 34 . Exercise: Weather, with dates . Recreate your weather series, using MMDD-style strings as your indexes. | Retrieve, via the index, the high temperature on August 5th | Retrieve, via the index, the high temperatures on August 4th through 9th. | What is the max temp going to be from August 9th through 11th? | s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . # if I want to use the positions to retrieve values, I use .iloc # .iloc and .loc are exactly the same if I don&#39;t have a special/new/custom index, if I just # use the default. # .iloc will always use integers, starting at 0. # .loc can use integers, but it can also use strings and other types . # &#39;0805&#39; not 0805 . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . # to s.index, you&#39;re set s.index = [2,4,6,8,10,12,14,16, 18, 20] . s . 2 33 4 34 6 34 8 34 10 34 12 34 14 33 16 32 18 32 20 32 dtype: int64 . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . Exercise: Weather, with dates . Recreate your weather series, using MMDD-style strings as your indexes. | Retrieve, via the index, the high temperature on August 5th | Retrieve, via the index, the high temperatures on August 4th through 9th. | What is the max temp going to be from August 9th through 11th? | # create the series # pass the keyword argument index with a list of strings s = Series([33, 34, 34c, 34, 34, 34, 33, 32, 32, 32], index=[&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ]) . s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32]) s.index = [&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ] . s.loc[&#39;0805&#39;] . 34 . s.loc[&#39;0804&#39;:&#39;0809&#39;] . 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 dtype: int64 . s.loc[&#39;0809&#39;:&#39;0811&#39;].max() . 32 . s.loc[&#39;0809&#39;:&#39;0811&#39;].describe() . count 3.0 mean 32.0 std 0.0 min 32.0 25% 32.0 50% 32.0 75% 32.0 max 32.0 dtype: float64 . Next up . Broadcasting | Mask arrays | More about indexes | Useful methods | 10-minute break . np.random.seed(0) s = Series(np.random.randint(0, 100, 10), index=list(&#39;abcdefghij&#39;)) s . a 44 b 47 c 64 d 67 e 67 f 9 g 83 h 21 i 36 j 87 dtype: int64 . s + s # we&#39;ll get a new series back, with + happening on each index . a 88 b 94 c 128 d 134 e 134 f 18 g 166 h 42 i 72 j 174 dtype: int64 . s + 10 # broadcasting -- each element in s ran +10 . a 54 b 57 c 74 d 77 e 77 f 19 g 93 h 31 i 46 j 97 dtype: int64 . s - 3 . a 41 b 44 c 61 d 64 e 64 f 6 g 80 h 18 i 33 j 84 dtype: int64 . s * 3 . a 132 b 141 c 192 d 201 e 201 f 27 g 249 h 63 i 108 j 261 dtype: int64 . s / 3 # truediv . a 14.666667 b 15.666667 c 21.333333 d 22.333333 e 22.333333 f 3.000000 g 27.666667 h 7.000000 i 12.000000 j 29.000000 dtype: float64 . s // 3 # floordiv . a 14 b 15 c 21 d 22 e 22 f 3 g 27 h 7 i 12 j 29 dtype: int64 . s ** 3 # to the 3rd power . a 85184 b 103823 c 262144 d 300763 e 300763 f 729 g 571787 h 9261 i 46656 j 658503 dtype: int64 . s % 3 # remainder = modulus . a 2 b 2 c 1 d 1 e 1 f 0 g 2 h 0 i 0 j 0 dtype: int64 . Exercise: Convert our temperatures . Assign s to be our 10-day forecast: . If your forecast is in Celsius, use broadcasting to get a new series in Fahrenheit | If your forecast is in Fahrenheit, use broadcasting to get a new series in Celsius | . # C = (°F − 32) x 5/9 . s . a 44 b 47 c 64 d 67 e 67 f 9 g 83 h 21 i 36 j 87 dtype: int64 . s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ]) . s . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . s * (9/5) + 32 . 0802 91.4 0803 93.2 0804 93.2 0805 93.2 0806 93.2 0807 93.2 0808 91.4 0809 89.6 0810 89.6 0811 89.6 dtype: float64 . (s * (9/5) + 32).describe() . count 10.000000 mean 91.760000 std 1.654086 min 89.600000 25% 90.050000 50% 92.300000 75% 93.200000 max 93.200000 dtype: float64 . f_temps = s * (9/5) + 32 . (f_temps - 32) * (5/9) . 0802 33.0 0803 34.0 0804 34.0 0805 34.0 0806 34.0 0807 34.0 0808 33.0 0809 32.0 0810 32.0 0811 32.0 dtype: float64 . s # s is an instance of Series, and it has access to the Series methods . 0802 33 0803 34 0804 34 0805 34 0806 34 0807 34 0808 33 0809 32 0810 32 0811 32 dtype: int64 . s.loc[[&#39;0802&#39;, &#39;0808&#39;, &#39;0806&#39;]] . 0802 33 0808 33 0806 34 dtype: int64 . s = Series([10, 20, 30, 40, 50]) s . 0 10 1 20 2 30 3 40 4 50 dtype: int64 . s.loc[[2, 3, 0]] . 2 30 3 40 0 10 dtype: int64 . # Pandas will return all elements of our series s, that correspond # to a True value. It will drop/ignore all elements that correspond # to a False value. s.loc[[True, False, True, True, False]] . 0 10 2 30 3 40 dtype: int64 . s . 0 10 1 20 2 30 3 40 4 50 dtype: int64 . s + 30 # this broadcasts with our series, giving us a new series, with everything +30 . 0 40 1 50 2 60 3 70 4 80 dtype: int64 . s &lt; 30 # this broadcasts the &lt; operator to our series, returning... a series of booleans . 0 True 1 True 2 False 3 False 4 False dtype: bool . s &gt; 30 . 0 False 1 False 2 False 3 True 4 True dtype: bool . s &lt; s.mean() . 0 True 1 True 2 False 3 False 4 False dtype: bool . # a series of booleans # what happens if I then apply that series of booleans as a mask index to my series? s.loc[s&lt;30] # what are the elements of s that are less than 30? . 0 10 1 20 dtype: int64 . s.loc[2:4] # this means: s, from index 2 to index 4 -- we&#39;re passing a slice object to s.loc[] . 2 30 3 40 4 50 dtype: int64 . s.loc[[2,3,4]] # this means, s, indexes 2,3, and 4 -- we&#39;re passing a list object to s.loc[] . 2 30 3 40 4 50 dtype: int64 . Mask indexes . If I want selected values from my series, a standard way to do this is with a &quot;mask index&quot;: . We create a boolean series, based on broadcasting a comparison operator (&lt;, ==, etc.) | We apply that boolean series as an index on the series. | . The result: We get only those values for which the comparison is True. . s . 0 10 1 20 2 30 3 40 4 50 dtype: int64 . s &lt; s.mean() . 0 True 1 True 2 False 3 False 4 False dtype: bool . s[s &lt; s.mean()] . 0 10 1 20 dtype: int64 . s[s &gt;= 15] . 1 20 2 30 3 40 4 50 dtype: int64 . np.random.seed(0) s = Series(np.random.randint(-50, 50, 10), index=list(&#39;abcdefghij&#39;)) s . a -6 b -3 c 14 d 17 e 17 f -41 g 33 h -29 i -14 j 37 dtype: int64 . s.loc[s &gt; 0] . c 14 d 17 e 17 g 33 j 37 dtype: int64 . s.loc[s &lt; 0] . a -6 b -3 f -41 h -29 i -14 dtype: int64 . s.loc[s%2 == 0] . a -6 c 14 i -14 dtype: int64 . s.loc[s%2 == 1] . b -3 d 17 e 17 f -41 g 33 h -29 j 37 dtype: int64 . Exercise: Family ages . Create a series in which the values are ages of people in your family, and the index contains their names. | Find all people who are below the mean age. | Find all people who are above the mean age + 1 std. | Find all people whose ages are odd. | s = Series([52, 50, 21, 19, 16], index=[&#39;RL&#39;, &#39;SF&#39;, &#39;AMLF&#39;, &#39;SBLF&#39;, &#39;ADLF&#39;]) s . RL 52 SF 50 AMLF 21 SBLF 19 ADLF 16 dtype: int64 . s.loc[s &lt; s.mean()] . AMLF 21 SBLF 19 ADLF 16 dtype: int64 . s.loc[s &gt; s.mean() + s.std()] . RL 52 SF 50 dtype: int64 . s.loc[s%2 == 1] . AMLF 21 SBLF 19 dtype: int64 . s.loc[s%2 == 1].index.values . array([&#39;AMLF&#39;, &#39;SBLF&#39;], dtype=object) . np.random.rand(0) s = Series(np.random.randint(0, 100, 100)) # 100 numbers, each between 0-100 s . 0 17 1 79 2 4 3 42 4 58 .. 95 19 96 46 97 42 98 56 99 60 Length: 100, dtype: int64 . # first, find s&gt;s.mean(), returning a boolean series (same index as s) # next, find s%2==0, returning a boolean series (same index as s) # use &amp; to get a new boolean series with the same index as s, where the values # are True if both input series are True s.loc[(s&gt;s.mean()) &amp; (s%2==0)] . 4 58 13 82 21 84 23 68 25 68 28 76 29 52 30 78 34 58 39 48 44 64 47 94 49 50 52 48 55 98 63 58 67 98 68 62 70 94 72 82 77 50 81 58 85 86 90 80 92 54 98 56 99 60 dtype: int64 . Combining conditions . If you want to get a new boolean series based on two existing boolean series: . Use &amp; for and, | for or, and ~ for not | Put () around every clause in your condition | Then apply the resulting boolean series on your series | . Exercise: Very big and very small numbers . Generate a series with 100 random integers (as I&#39;ve done, using np.random.randint) from 0-1000. | Find the numbers that are either very big (i.e., &gt; mean + std) or very small (i.e., &lt; mean - std) | np.random.seed(0) s = Series(np.random.randint(0, 1000, 100)) s . 0 684 1 559 2 629 3 192 4 835 ... 95 398 96 611 97 565 98 908 99 633 Length: 100, dtype: int64 . s &gt; s.mean() + s.std() . 0 False 1 False 2 False 3 False 4 True ... 95 False 96 False 97 False 98 True 99 False Length: 100, dtype: bool . s &lt; s.mean() - s.std() . 0 False 1 False 2 False 3 True 4 False ... 95 False 96 False 97 False 98 False 99 False Length: 100, dtype: bool . s.loc[(s &gt; s.mean() + s.std()) | (s &lt; s.mean() - s.std())] . 3 192 4 835 8 9 14 70 22 87 23 174 25 849 28 845 29 72 31 916 32 115 33 976 36 847 39 850 40 99 41 984 42 177 46 147 47 910 50 961 58 151 62 882 63 183 64 28 66 128 67 128 68 932 69 53 70 901 78 42 81 888 84 999 85 937 86 57 88 870 89 119 92 82 93 91 94 896 98 908 dtype: int64 . s.loc[(s &gt; s.mean() + 2 * s.std()) | (s &lt; s.mean() - 2 * s.std())] . Series([], dtype: int64) . Next up . More on indexes | Useful methods on our series | np.random.seed(0) s = Series(np.random.randint(0, 1000, 10), index=list(&#39;abcdefghij&#39;)) s . a 684 b 559 c 629 d 192 e 835 f 763 g 707 h 359 i 9 j 723 dtype: int64 . t = Series(np.random.randint(0, 1000, 10), index=list(&#39;jihgfedcba&#39;)) t . j 277 i 754 h 804 g 599 f 70 e 472 d 600 c 396 b 314 a 705 dtype: int64 . s + t . a 1389 b 873 c 1025 d 792 e 1307 f 833 g 1306 h 1163 i 763 j 1000 dtype: int64 . 684+705 . 1389 . t = Series(np.random.randint(0, 1000, 10), index=list(&#39;abcdeabcde&#39;)) t . a 777 b 916 c 115 d 976 e 755 a 709 b 847 c 431 d 448 e 850 dtype: int64 . t.loc[&#39;a&#39;] . a 777 a 709 dtype: int64 . t.loc[[&#39;a&#39;, &#39;c&#39;]] . a 777 a 709 c 115 c 431 dtype: int64 . s + t . a 1461.0 a 1393.0 b 1475.0 b 1406.0 c 744.0 c 1060.0 d 1168.0 d 640.0 e 1590.0 e 1685.0 f NaN g NaN h NaN i NaN j NaN dtype: float64 . t . a 777 b 916 c 115 d 976 e 755 a 709 b 847 c 431 d 448 e 850 dtype: int64 . t.iloc[5] . 709 . t.loc[&#39;a&#39;:&#39;c&#39;] . KeyError Traceback (most recent call last) /var/folders/rr/0mnyyv811fs5vyp22gf4fxk00000gn/T/ipykernel_3159/4171544057.py in &lt;module&gt; 1 # what if I try to get t.loc[&#39;a&#39;:&#39;c&#39;]? 2 -&gt; 3 t.loc[&#39;a&#39;:&#39;c&#39;] /usr/local/lib/python3.10/site-packages/pandas/core/indexing.py in __getitem__(self, key) 965 966 maybe_callable = com.apply_if_callable(key, self.obj) --&gt; 967 return self._getitem_axis(maybe_callable, axis=axis) 968 969 def _is_scalar_access(self, key: tuple): /usr/local/lib/python3.10/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis) 1178 if isinstance(key, slice): 1179 self._validate_key(key, axis) -&gt; 1180 return self._get_slice_axis(key, axis=axis) 1181 elif com.is_bool_indexer(key): 1182 return self._getbool_axis(key, axis=axis) /usr/local/lib/python3.10/site-packages/pandas/core/indexing.py in _get_slice_axis(self, slice_obj, axis) 1212 1213 labels = obj._get_axis(axis) -&gt; 1214 indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step) 1215 1216 if isinstance(indexer, slice): /usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py in slice_indexer(self, start, end, step, kind) 6272 self._deprecated_arg(kind, &#34;kind&#34;, &#34;slice_indexer&#34;) 6273 -&gt; 6274 start_slice, end_slice = self.slice_locs(start, end, step=step) 6275 6276 # return a slice /usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py in slice_locs(self, start, end, step, kind) 6482 start_slice = None 6483 if start is not None: -&gt; 6484 start_slice = self.get_slice_bound(start, &#34;left&#34;) 6485 if start_slice is None: 6486 start_slice = 0 /usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind) 6409 slc = lib.maybe_booleans_to_slice(slc.view(&#34;u1&#34;)) 6410 if isinstance(slc, np.ndarray): -&gt; 6411 raise KeyError( 6412 f&#34;Cannot get {side} slice bound for non-unique &#34; 6413 f&#34;label: {repr(original_label)}&#34; KeyError: &#34;Cannot get left slice bound for non-unique label: &#39;a&#39;&#34; . t.loc[[&#39;a&#39;, &#39;c&#39;, &#39;d&#39;]] . a 777 a 709 c 115 c 431 d 976 d 448 dtype: int64 . t.loc[[&#39;a&#39;, &#39;c&#39;, &#39;d&#39;]].describe() . count 6.000000 mean 576.000000 std 305.947708 min 115.000000 25% 435.250000 50% 578.500000 75% 760.000000 max 976.000000 dtype: float64 . Index types . Default is numeric, starting at 0, and going until the length - 1 (much like Python lists, strings, tuples). | You can also have a range index, specified with range(start, end) or range(start, end, step). | You can have a string index, specified with a list of strings. | . s.index . Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;], dtype=&#39;object&#39;) . t.index . Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;) . t.index = range(10, 30, 2) . t . 10 777 12 916 14 115 16 976 18 755 20 709 22 847 24 431 26 448 28 850 dtype: int64 . t.index . RangeIndex(start=10, stop=30, step=2) . t . 10 777 12 916 14 115 16 976 18 755 20 709 22 847 24 431 26 448 28 850 dtype: int64 . t.index = list(&#39;abcdeabcde&#39;) t . a 777 b 916 c 115 d 976 e 755 a 709 b 847 c 431 d 448 e 850 dtype: int64 . t.loc[&#39;e&#39;] = 5555 # this will assign to two elements t . a 777 b 916 c 115 d 976 e 5555 a 709 b 847 c 431 d 448 e 5555 dtype: int64 . s . a 684 b 559 c 629 d 192 e 835 f 763 g 707 h 359 i 9 j 723 dtype: int64 . s.loc[s&lt;s.mean()] = 50 # we can assign here, using our boolean index . s . a 684 b 559 c 629 d 50 e 835 f 763 g 707 h 50 i 50 j 723 dtype: int64 . Exercise: Weekend temps . Recreate our series of high temperatures, but instead of dates as the index, use day names (Mon, Tue). | What will be the mean temperature on weekends (Sat-Sun). | What will be the mean temperature on weekdays? | s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;, &#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39; ]) . s.loc[[&#39;Sat&#39;, &#39;Sun&#39;]].mean() . 34.0 . s.loc[[&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;]] . Mon 33 Tue 33 Tue 32 Wed 34 Wed 32 Thu 34 Thu 32 Fri 34 dtype: int64 . # we can run the &quot;isin&quot; method on a series, which returns True/False for each value # here, we&#39;re saying: get a boolean series indicating whether each value of s&#39;s index # is in [&#39;Sat&#39;, &#39;Sun&#39;] # apply that as a mask index on s s.loc[s.index.isin([&#39;Sat&#39;, &#39;Sun&#39;])] . Sat 34 Sun 34 dtype: int64 . # here, we do the same as above, but flip the logic, so that we get # the opposite elements s.loc[~s.index.isin([&#39;Sat&#39;, &#39;Sun&#39;])] . Tue 33 Wed 34 Thu 34 Fri 34 Mon 33 Tue 32 Wed 32 Thu 32 dtype: int64 . temps = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;, &#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39; ]) . np.random.seed(0) s = Series(np.random.randint(0, 1000, 100)) s . 0 684 1 559 2 629 3 192 4 835 ... 95 398 96 611 97 565 98 908 99 633 Length: 100, dtype: int64 . # I can use the head and tail methods. By default, they show 5 elements, but # can pass an integer to change that s.head() . 0 684 1 559 2 629 3 192 4 835 dtype: int64 . s.tail() . 95 398 96 611 97 565 98 908 99 633 dtype: int64 . s.head(7) . 0 684 1 559 2 629 3 192 4 835 5 763 6 707 dtype: int64 . # the value_counts method does this for us # if I call temps.value_counts(), each of the distinct values in temps becomes # an index element, and the number of times it appears becomes the value # it&#39;s ordered, from most to least common # it&#39;s a series, so we can do series things on it -- like .head() temps.value_counts() . 34 5 32 3 33 2 dtype: int64 . temps.value_counts(normalize=True) . 34 0.5 32 0.3 33 0.2 dtype: float64 . s.unique() # returns a NumPy array! . array([684, 559, 629, 192, 835, 763, 707, 359, 9, 723, 277, 754, 804, 599, 70, 472, 600, 396, 314, 705, 486, 551, 87, 174, 849, 677, 537, 845, 72, 777, 916, 115, 976, 755, 709, 847, 431, 448, 850, 99, 984, 177, 797, 659, 147, 910, 423, 288, 961, 265, 697, 639, 544, 543, 714, 244, 151, 675, 510, 459, 882, 183, 28, 802, 128, 932, 53, 901, 550, 488, 756, 273, 335, 388, 617, 42, 442, 888, 257, 321, 999, 937, 57, 291, 870, 119, 779, 430, 82, 91, 896, 398, 611, 565, 908, 633]) . temps.unique() . array([33, 34, 32]) . temps.value_counts().index . Int64Index([34, 32, 33], dtype=&#39;int64&#39;) . s = Series([52, 50, 21, 19, 16], index=[&#39;RL&#39;, &#39;SF&#39;, &#39;AMLF&#39;, &#39;SBLF&#39;, &#39;ADLF&#39;]) s . RL 52 SF 50 AMLF 21 SBLF 19 ADLF 16 dtype: int64 . s.diff() . RL NaN SF -2.0 AMLF -29.0 SBLF -2.0 ADLF -3.0 dtype: float64 . s.pct_change() # silly with ages! . RL NaN SF -0.038462 AMLF -0.580000 SBLF -0.095238 ADLF -0.157895 dtype: float64 . Exercise: Change in weather . Using our high-temperature series, what are the 3 most common temperatures in your 10-day forecats? | On what day will there be the greatest numeric (not percentage) change in temperature? How about the smallest change? | s = Series([33, 34, 34, 34, 34, 34, 33, 32, 32, 32], index=[&#39;0802&#39;, &#39;0803&#39;, &#39;0804&#39;, &#39;0805&#39;, &#39;0806&#39;, &#39;0807&#39;, &#39;0808&#39;, &#39;0809&#39;, &#39;0810&#39;, &#39;0811&#39; ]) . s.value_counts() . 34 5 32 3 33 2 dtype: int64 . s.value_counts().head(2) . 34 5 32 3 dtype: int64 . s.diff() . 0802 NaN 0803 1.0 0804 0.0 0805 0.0 0806 0.0 0807 0.0 0808 -1.0 0809 -1.0 0810 0.0 0811 0.0 dtype: float64 . s.loc[s.diff() == s.diff().max()] . 0803 34 dtype: int64 . s.loc[s.diff() == s.diff().min()] . 0808 33 0809 32 dtype: int64 . Next week: . dtypes -- what are they, and why do we care? | Data frames (2D data) |",
            "url": "https://leungadh.github.io/coding/2022/08/05/Python-analytics-main.html",
            "relUrl": "/2022/08/05/Python-analytics-main.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Identify the members of the band Mirror",
            "content": "From Model to Production . The Practice of Deep Learning . Starting Your Project . The State of Deep Learning . Computer vision . Text (natural language processing) . Combining text and images . Tabular data . Recommendation systems . Other data types . The Drivetrain Approach . Gathering Data . clean . To download images with Bing Image Search, sign up at Microsoft Azure for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;febeb1b830924132a317e052e512f951&#39;) . key . &#39;febeb1b830924132a317e052e512f951&#39; . search_images_bing . &lt;function fastbook.search_images_bing&gt; . results = search_images_bing(key, &#39;mirror keung to&#39;) ims = results.attrgot(&#39;contentUrl&#39;) len(ims) . 150 . ims . (#150) [&#39;https://i.pinimg.com/originals/07/70/63/077063222921ead83cc6b1ca1636407c.jpg&#39;,&#39;https://media.karousell.com/media/photos/products/2019/10/04/mirror_keung_to_yes_card_59__1570147174_bd46b908.jpg&#39;,&#39;https://i.pinimg.com/736x/34/fa/e0/34fae0f3a8a41b8b178026971149e3f3.jpg&#39;,&#39;https://852hk.com/wp-content/uploads/2021/01/852hk-keung-to.jpg&#39;,&#39;https://i.pinimg.com/originals/b6/4f/32/b64f32b83ca2c039de5ae04c40eeb28c.jpg&#39;,&#39;https://i.pinimg.com/originals/5c/83/39/5c8339b56fb8825a6c9c1c9b4156007d.jpg&#39;,&#39;http://kpopmembersbio.com/wp-content/uploads/2021/05/Keung-To.jpg&#39;,&#39;https://i.pinimg.com/originals/44/c8/da/44c8dad457a72284fc66020595c8c976.jpg&#39;,&#39;https://i.pinimg.com/originals/99/13/6a/99136a09150cd5232487b8c3ed74c1f4.jpg&#39;,&#39;https://i.pinimg.com/736x/36/51/25/3651250e453b47ddc6ae7923a1a159c3.jpg&#39;...] . dest = &#39;images/keungto.jpg&#39; download_url(ims[0], dest) . . 102.15% [155648/152373 00:00&lt;00:00] Path(&#39;images/keungto.jpg&#39;) . im = Image.open(dest) im.to_thumb(128,128) . member_names = &#39;Keung_To&#39;,&#39;Anson_Lo&#39;,&#39;Edan_Lui&#39; path = Path(&#39;mirror_members&#39;) . if not path.exists(): path.mkdir() for o in member_names: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} mirror&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . fns = get_image_files(path) fns . (#437) [Path(&#39;mirror_members/Keung_To/00000099.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000146.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000005.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000074.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000059.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000093.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000109.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000001.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000130.jpg&#39;),Path(&#39;mirror_members/Keung_To/00000089.jpg&#39;)...] . failed = verify_images(fns) failed . (#1) [Path(&#39;mirror_members/Keung_To/00000018.jpg&#39;)] . failed.map(Path.unlink); . Sidebar: Getting Help in Jupyter Notebooks . End sidebar . From Data to DataLoaders . mirrors = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = mirrors.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . mirrors = mirrors.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = mirrors.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . mirrors = mirrors.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = mirrors.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . mirrors = mirrors.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = mirrors.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . mirrors = mirrors.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = mirrors.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . mirrors = mirrors.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = mirrors.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.827225 | 1.942311 | 0.682353 | 00:13 | . epoch train_loss valid_loss error_rate time . 0 | 1.586008 | 1.490312 | 0.705882 | 00:13 | . 1 | 1.416846 | 1.422595 | 0.552941 | 00:13 | . 2 | 1.258077 | 1.415280 | 0.564706 | 00:12 | . 3 | 1.175401 | 1.406915 | 0.517647 | 00:13 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;images/keungto.jpg&#39;) . (&#39;Keung_To&#39;, TensorBase(2), TensorBase([0.1231, 0.0560, 0.8209])) . learn_inf.dls.vocab . [&#39;Anson_Lo&#39;, &#39;Edan_Lui&#39;, &#39;Keung_To&#39;] . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . btn_upload.data . [&#39;images/keungto.jpg&#39;] . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . Deploying your app . How to Avoid Disaster . Unforeseen Consequences and Feedback Loops . Get Writing! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out-of-domain data&quot;? | What is &quot;domain shift&quot;? | What are the three steps in the deployment process? | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://leungadh.github.io/coding/2022/04/15/Mirror-Images-v1.html",
            "relUrl": "/2022/04/15/Mirror-Images-v1.html",
            "date": " • Apr 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "From Model to Production",
            "content": "FastAI Lecture 3: Bear classification . ipython notebook 2, production . The Practice of Deep Learning . Starting Your Project . The State of Deep Learning . Computer vision . Text (natural language processing) . Combining text and images . Tabular data . Recommendation systems . Other data types . The Drivetrain Approach . Gathering Data . clean . To download images with Bing Image Search, sign up at Microsoft Azure for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;febeb1b830924132a317e052e512f951&#39;) . search_images_bing . &lt;function fastbook.search_images_bing&gt; . results = search_images_bing(key, &#39;grizzly bear&#39;) ims = results.attrgot(&#39;contentUrl&#39;) len(ims) . 150 . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . . 100.99% [704512/697626 00:00&lt;00:00] Path(&#39;images/grizzly.jpg&#39;) . im = Image.open(dest) im.to_thumb(128,128) . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . fns = get_image_files(path) fns . (#431) [Path(&#39;bears/black/00000145.jpg&#39;),Path(&#39;bears/black/00000063.jpg&#39;),Path(&#39;bears/black/00000125.jpg&#39;),Path(&#39;bears/black/00000075.jpg&#39;),Path(&#39;bears/black/00000060.jpg&#39;),Path(&#39;bears/black/00000046.jpg&#39;),Path(&#39;bears/black/00000135.jpg&#39;),Path(&#39;bears/black/00000132.jpg&#39;),Path(&#39;bears/black/00000098.JPG&#39;),Path(&#39;bears/black/00000079.jpg&#39;)...] . failed = verify_images(fns) failed . (#4) [Path(&#39;bears/black/00000107.jpg&#39;),Path(&#39;bears/grizzly/00000067.jpg&#39;),Path(&#39;bears/teddy/00000035.jpg&#39;),Path(&#39;bears/teddy/00000065.jpg&#39;)] . failed.map(Path.unlink); . Sidebar: Getting Help in Jupyter Notebooks . End sidebar . From Data to DataLoaders . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = bears.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 1.478040 | 0.588613 | 0.247059 | 00:34 | . epoch train_loss valid_loss error_rate time . 0 | 0.195702 | 0.156654 | 0.047059 | 00:35 | . 1 | 0.144840 | 0.067281 | 0.023529 | 00:33 | . 2 | 0.102226 | 0.082883 | 0.023529 | 00:33 | . 3 | 0.077645 | 0.091636 | 0.023529 | 00:33 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorBase(1), TensorBase([2.2828e-06, 1.0000e+00, 1.6265e-07])) . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . img . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . Deploying your app . How to Avoid Disaster . Unforeseen Consequences and Feedback Loops . Get Writing! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out-of-domain data&quot;? | What is &quot;domain shift&quot;? | What are the three steps in the deployment process? | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://leungadh.github.io/coding/2022/04/08/fastai-02-production.html",
            "relUrl": "/2022/04/08/fastai-02-production.html",
            "date": " • Apr 8, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep learning with fast.ai cookbook - MNIST "hello world" example",
            "content": "!pip install -Uqq fastbook import fastbook from fastbook import * from fastai.vision.all import * . |████████████████████████████████| 720 kB 5.3 MB/s |████████████████████████████████| 1.2 MB 36.5 MB/s |████████████████████████████████| 189 kB 47.9 MB/s |████████████████████████████████| 48 kB 4.9 MB/s |████████████████████████████████| 55 kB 3.5 MB/s |████████████████████████████████| 561 kB 44.7 MB/s |████████████████████████████████| 51 kB 291 kB/s |████████████████████████████████| 130 kB 46.3 MB/s . fastbook.setup_book() . Mounted at /content/gdrive . # if the dataset has not been copied there already path = untar_data(URLs.MNIST) . . 100.03% [15687680/15683414 00:00&lt;00:00] path.ls() . (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . %%time # create an image dataloaders object using the path # note that because of the directory structure of the dataset # the train and valid sets have to be explicitly specified # details here: https://github.com/fastai/fastai/issues/1129 dls = ImageDataLoaders.from_folder(path, train=&#39;training&#39;, valid=&#39;testing&#39;) # create a learner object using the dataloaders that was just defined # architecture is resnet18; see https://pytorch.org/hub/pytorch_vision_resnet/ # loss function is selected for multi class classification # accuracy is the metric used to optimize learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy) # fit the model for one epoch using 1cycle policy # see https://docs.fast.ai/callback.schedule.html#Learner.fit_one_cycle learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.557135 | 0.525823 | 0.990400 | 02:00 | . CPU times: user 1min 22s, sys: 3.96 s, total: 1min 26s Wall time: 2min 19s . dls.train.show_batch(max_n=4, nrows=1) . dls.valid.show_batch(max_n=4, nrows=1) . img_files = get_image_files(path/&quot;testing&quot;) img = PILImage.create(img_files[7000]) img . interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(4, nrows=1) . learn.summary() . Sequential (Input shape: 64 x 3 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 14 x 14 Conv2d 9408 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 64 x 64 x 7 x 7 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 64 x 128 x 4 x 4 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 64 x 256 x 2 x 2 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 64 x 512 x 1 x 1 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True AdaptiveAvgPool2d AdaptiveMaxPool2d ____________________________________________________________________________ 64 x 1024 Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 64 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 64 x 10 Linear 5120 True ____________________________________________________________________________ Total params: 11,708,992 Total trainable params: 11,708,992 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f0e785fab00&gt; Loss function: LabelSmoothingCrossEntropy() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . img = PILImage.create(img_files[0]) img . learn.predict(img) . (&#39;6&#39;, TensorBase(6), TensorBase([0.0107, 0.0093, 0.0098, 0.0088, 0.0116, 0.0074, 0.9145, 0.0091, 0.0094, 0.0094])) . img = PILImage.create(img_files[2030]) img . learn.predict(img) . (&#39;0&#39;, TensorBase(0), TensorBase([0.9077, 0.0100, 0.0113, 0.0104, 0.0109, 0.0092, 0.0092, 0.0092, 0.0086, 0.0134])) . img = PILImage.create(img_files[5800]) img . learn.predict(img) . (&#39;5&#39;, TensorBase(5), TensorBase([0.0094, 0.0095, 0.0090, 0.0089, 0.0097, 0.9215, 0.0103, 0.0082, 0.0063, 0.0071])) .",
            "url": "https://leungadh.github.io/coding/2022/03/23/mnist_hello_world.html",
            "relUrl": "/2022/03/23/mnist_hello_world.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Notebook to validate Gradient setup",
            "content": "import fastai fastai.__version__ . &#39;2.1.5&#39; . !nvidia-smi . Sat Jul 3 22:40:46 2021 +--+ | NVIDIA-SMI 450.36.06 Driver Version: 450.36.06 CUDA Version: 11.0 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro P4000 On | 00000000:00:05.0 Off | N/A | | 46% 41C P0 34W / 105W | 853MiB / 8119MiB | 35% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ .",
            "url": "https://leungadh.github.io/coding/colab/setup/2022/03/22/validate-gradient-setup.html",
            "relUrl": "/colab/setup/2022/03/22/validate-gradient-setup.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Line Plot",
            "content": "Python Line Plot . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . Line plots . Showing the line plot of X and Y value . x:The x-coordinate | y:The y-coordinate | . k = np.linspace(0,10) #np.linspace (start, finish) equally divine into 50 numbers(default) kk = np.sin(x) k, kk . (array([ 0. , 0.20408163, 0.40816327, 0.6122449 , 0.81632653, 1.02040816, 1.2244898 , 1.42857143, 1.63265306, 1.83673469, 2.04081633, 2.24489796, 2.44897959, 2.65306122, 2.85714286, 3.06122449, 3.26530612, 3.46938776, 3.67346939, 3.87755102, 4.08163265, 4.28571429, 4.48979592, 4.69387755, 4.89795918, 5.10204082, 5.30612245, 5.51020408, 5.71428571, 5.91836735, 6.12244898, 6.32653061, 6.53061224, 6.73469388, 6.93877551, 7.14285714, 7.34693878, 7.55102041, 7.75510204, 7.95918367, 8.16326531, 8.36734694, 8.57142857, 8.7755102 , 8.97959184, 9.18367347, 9.3877551 , 9.59183673, 9.79591837, 10. ]), array([ 0. , 0.39692415, 0.72863478, 0.94063279, 0.99808748, 0.89155923, 0.63855032, 0.2806294 , -0.12339814, -0.50715171, -0.80758169, -0.97532829, -0.9828312 , -0.82885774, -0.53870529, -0.16004509, 0.24491007, 0.6096272 , 0.8741843 , 0.99511539, 0.95255185, 0.75348673, 0.43062587, 0.0370144 , -0.36267843, -0.70278422, -0.92742455, -0.99969166, -0.90771225, -0.66659829, -0.31596412, 0.08658207, 0.47490306, 0.78519883, 0.96648865, 0.98898712, 0.8489978 , 0.56952055, 0.19647269, -0.20885508, -0.57986856, -0.85561127, -0.99077947, -0.9631654 , -0.77730599, -0.4637374 , -0.07397807, 0.32793565, 0.67597047, 0.91294525])) . x = np.linspace(0,20) y1 = np.sin(x) y2 = np.sin(x - np.pi) plt.figure() plt.plot(x,y1) plt.plot(x,y2) plt.figure; . Implementing plot with . Color | Linestyle | Linewidth | Marker, Marker size | Label | Legend | . x = np.linspace(0, 20) y1 = np.sin(x) y2 = np.sin(x - np.pi) plt.figure() plt.plot(x, y1, color=&#39;black&#39;, linestyle=&#39;-&#39;, linewidth=2, marker=&#39;s&#39;, markersize=6, label=&#39;y1&#39;) plt.plot(x, y2, color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2, marker=&#39;^&#39;, markersize=6, label=&#39;y2&#39;) plt.legend() ; . &#39;&#39; .",
            "url": "https://leungadh.github.io/coding/python/visual/2022/03/20/Python-Line.html",
            "relUrl": "/python/visual/2022/03/20/Python-Line.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "My First Post",
            "content": "Here is the date . Date:March 8, 2022 . Third sub-title . Testing the 3rd-sub-title .",
            "url": "https://leungadh.github.io/coding/jupyter/2022/03/08/First-Post.html",
            "relUrl": "/jupyter/2022/03/08/First-Post.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://leungadh.github.io/coding/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://leungadh.github.io/coding/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://leungadh.github.io/coding/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://leungadh.github.io/coding/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}